{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CH 1"
      ],
      "metadata": {
        "id": "ESXbK1mfAPdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 자연어 처리 모델\n",
        "- 자연어 처리 모델은 자연어를 입력받아서 해당 입력이 특정 범주일 확률을 반환하는 확률 함수\n",
        "- 그 중에서도 딥러닝을 이용한 자연어 처리 모델\n",
        "  - BERT (Bidirectional Encoder Representations from Transformers)\n",
        "  - GPT (Generative Pre-trained Transformer)"
      ],
      "metadata": {
        "id": "Vjfmg_M35dWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer leaning (전이 학습)\n",
        "- 특정 태스크를 학습한 모델을 다른 태스크 수행에 재사용하는 기법\n",
        "- 기존보다 모델의 학습 속도가 빨라지고 새로운 태스크를 더 잘 수행하는 경향이 있다\n",
        "  - Upstream task : 기존의 태스크, 대규모 말뭉치의 문맥을 이해하는 과제\n",
        "    - Pretrain : Upstream task를 학습하는 과정\n",
        "  - Downstream task : 새로운 태스크, 풀고자 하는 자연어 처리의 구체적인 문제들\n",
        "  "
      ],
      "metadata": {
        "id": "gBfo3CLG6a35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upsteram task\n",
        "1. 다음 단어 맞히기\n",
        "  - GPT 계열\n",
        "  - Language Model\n",
        "2. 빈칸 채우기\n",
        "  - BERT 계열\n",
        "  - Masked Language Model"
      ],
      "metadata": {
        "id": "Wk-CWNAF7JYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downstream task\n",
        "- Classification을 수행\n",
        "- Fine tunning : Downstream task의 학습 방식 중 하나\n",
        "  - Pretrain을 마친 모델을 Downstream task에 맞게 업데이트하는 기법\n",
        "  - 이 외에도 Prompt tunning, In context tunning 존재\n",
        "1. 문서 분류\n",
        "2. 자연어 추론\n",
        "3. 개체명 인식\n",
        "4. 질의응답\n",
        "5. 문장 생성"
      ],
      "metadata": {
        "id": "7CXyGdgV754D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CH 2"
      ],
      "metadata": {
        "id": "qCcDqqXiAS9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- 문장을 토큰 시퀀스로 나누는 과정\n",
        "- Tokenizer : 토큰화를 수행하는 프로그램\n",
        "1. 단어 단위 토큰화 : 어휘 집합의 크기가 매우 커짐\n",
        "2. 문자 단위 토큰화 : 의미 있는 단어가 되기 어려움\n",
        "3. 서브워드 단위 토큰화 : 단어와 문자 단위 토큰화의 중간에 있는 형태\n",
        "  - 바이트 페어 인코딩"
      ],
      "metadata": {
        "id": "6TJB7cgXAVDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Byte Pair Encoding (BPE,바이트 페어 인코딩)\n",
        "- 사전 크기 증가를 억제하면서도 정보를 효율적으로 압축할 수 있음\n",
        "- 말뭉치에서 자주 나타나는 문자열(서브워드)을 토큰으로 분석함\n",
        "1. 어휘 집합 구축\n",
        "  - 자주 등장하는 문자열을 병합하고 이를 어휘 집합에 추가. 이를 원하는 어휘 집합 크기가 될 때가지 반복\n",
        "  - Pre-tokenize : 말뭉치의 모든 문장을 공백으로 나눔\n",
        "2. 토큰화 \n",
        "  - 토큰화 대상 문장의 각 어절에서 어휘 집합에 있는 서브워드가 포함되었을 때 해당 서브워드를 어절에서 분리"
      ],
      "metadata": {
        "id": "A_-8P0GnBE7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wordpiece\n",
        "- 말뭉치에서 자주 등장한 문자열을 토큰으로 인식한다는 점에서 BPE와 본질적으로 유사함\n",
        "- 하지만 wordpiece는 단순히 빈도를 기준으로 병합하는 것이 아니라, 병합했을 때 말뭉치의 '우도'를 가장 높이는 쌍을 병합\n",
        "- BPE와 달리 merge.txt 생성하지 않음, vocab.txt만 사용"
      ],
      "metadata": {
        "id": "e-uE-CKhC-FM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer 생성"
      ],
      "metadata": {
        "id": "1iVYwoNzEKOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ratsnlp"
      ],
      "metadata": {
        "id": "zfNkyaSB97Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2A0WDs8EQcZ",
        "outputId": "ff5b1662-0510-436c-f829-0d0d7cd4f032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "nsmc = Korpora.load('nsmc', force_download=True)"
      ],
      "metadata": {
        "id": "sjtydg6OEZc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NSMC 전치\n",
        "import os\n",
        "def write_lines(path, lines):\n",
        "  with open(path, 'w', encoding='utf-8') as f:\n",
        "    for line in lines:\n",
        "      f.write(f'{line}\\n')\n",
        "\n",
        "write_lines('/root/train.txt', nsmc.train.get_all_texts())\n",
        "write_lines('/root/test.txt', nsmc.test.get_all_texts())"
      ],
      "metadata": {
        "id": "L9O8QQEAEiAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디렉토리 생성\n",
        "import os\n",
        "os.makedirs('/gdrive/My Drive/nlpbook/bbpe', exist_ok=True)"
      ],
      "metadata": {
        "id": "7CMxonOGE35H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 바이트 수준 BPE 어휘 집합 구축\n",
        "# GPT 토크나이저 - BPE\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "bytebpe_tokenizer = ByteLevelBPETokenizer()\n",
        "bytebpe_tokenizer.train(\n",
        "    files=['/root/train.txt','/root/test.txt'], # 학습 말뭉치를 리스트 형태로 넣기\n",
        "    vocab_size = 10000, # 어휘 집합 ㅡ기 조절\n",
        "    special_tokens=['[PAD]'] # 특수 토큰 추가\n",
        ")\n",
        "bytebpe_tokenizer.save_model('/gdrive/My Drive/nlpbook/bbpe')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpuY-YpPFLRZ",
        "outputId": "9d7e9f46-8e52-4153-d795-32335ade01ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/gdrive/My Drive/nlpbook/bbpe/vocab.json',\n",
              " '/gdrive/My Drive/nlpbook/bbpe/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT 토크나이저 - Wordpiece\n",
        "import os\n",
        "os.makedirs('/gdrive/My Drive/nlpbook/wordpiece',exist_ok=True)\n",
        "\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
        "wordpiece_tokenizer.train(\n",
        "    files=['/root/train.txt', '/root/test.txt'],\n",
        "    vocab_size=10000\n",
        ")\n",
        "\n",
        "wordpiece_tokenizer.save_model('/gdrive/My Drive/nlpbook/wordpiece')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo3cmDPrFtb8",
        "outputId": "0d2210d5-325b-48ed-b0ca-d3b7c821b3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/gdrive/My Drive/nlpbook/wordpiece/vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토큰화하기"
      ],
      "metadata": {
        "id": "c7YbYZudGsWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT 토크나이저 선언\n",
        "from transformers import GPT2Tokenizer\n",
        "tokenizer_gpt = GPT2Tokenizer.from_pretrained('/gdrive/My Drive/nlpbook/bbpe')\n",
        "tokenizer_gpt.pad_token = '[PAD]'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSvGXVjrGjk-",
        "outputId": "b471b0bb-05d5-4cb7-ee29-d21a24847db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file /gdrive/My Drive/nlpbook/bbpe/config.json not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT 토크나이저로 토큰화하기\n",
        "sentences = [\n",
        "    '아 더빙.. 진짜 짜증나네요 목소리',\n",
        "    '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나',\n",
        "    '별루 였다..'\n",
        "]\n",
        "tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "FAP__Lh6HAYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkbcTPQzHUSe",
        "outputId": "0e643ab2-8089-40ef-c15b-d34867806546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ìķĦ', 'ĠëįĶë¹Ļ', '..', 'Ġì§Ħì§ľ', 'Ġì§ľì¦ĿëĤĺ', 'ëĦ¤ìļĶ', 'Ġëª©ìĨĮë¦¬'],\n",
              " ['íĿł',\n",
              "  '...',\n",
              "  'íı¬ìĬ¤íĦ°',\n",
              "  'ë³´ê³ł',\n",
              "  'Ġì´ĪëĶ©',\n",
              "  'ìĺģíĻĶ',\n",
              "  'ì¤Ħ',\n",
              "  '....',\n",
              "  'ìĺ¤ë²Ħ',\n",
              "  'ìĹ°ê¸°',\n",
              "  'ì¡°ì°¨',\n",
              "  'Ġê°Ģë³į',\n",
              "  'ì§Ģ',\n",
              "  'ĠìķĬ',\n",
              "  'êµ¬ëĤĺ'],\n",
              " ['ë³Ħë£¨', 'Ġìĺ', 'Ģëĭ¤', '..']]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT 모델 입력 만들기\n",
        "batch_inputs = tokenizer_gpt(\n",
        "    sentences,\n",
        "    padding='max_length', # 문장의 최대 길이에 맞춰 패딩\n",
        "    max_length = 12, # 문장의 토큰 기준 최대 길이\n",
        "    truncation = True, # 문장 잘림 허용 옵션\n",
        ")"
      ],
      "metadata": {
        "id": "cXn9HyLJHV4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs['input_ids'] # 토큰 인덱스 시퀀스"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs2SAWfdHq7X",
        "outputId": "94c186b4-b14f-4e8b-fbf2-dd776150d933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[334, 2338, 263, 581, 4055, 464, 3808, 0, 0, 0, 0, 0],\n",
              " [3693, 336, 2876, 758, 2883, 356, 806, 422, 9875, 875, 2960, 7292],\n",
              " [4957, 451, 3653, 263, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs['attention_mask'] # 일반 토큰(1), 패딩 토큰(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78Coc4MbHtvz",
        "outputId": "cce17613-7f72-4838-a7ac-76e6a6739ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT 토크나이저 선언\n",
        "from transformers import BertTokenizer\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(\n",
        "    '/gdrive/My Drive/nlpbook/wordpiece',\n",
        "    do_lower_case = False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA2UkjBtHz4n",
        "outputId": "6ebbe729-6d12-45bd-b9e8-3276289c0711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "file /gdrive/My Drive/nlpbook/wordpiece/config.json not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT 토크나이저로 토큰화하기\n",
        "sentences = [\n",
        "    '아 더빙.. 진짜 짜증나네요 목소리',\n",
        "    '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나',\n",
        "    '별루 였다..'\n",
        "]\n",
        "tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "MgKiahAyIIi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzLqPFcSIk3L",
        "outputId": "94610f44-2224-4c4e-a86a-b95d964e49cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['아', '더빙', '.', '.', '진짜', '짜증나', '##네요', '목소리'],\n",
              " ['흠',\n",
              "  '.',\n",
              "  '.',\n",
              "  '.',\n",
              "  '포스터',\n",
              "  '##보고',\n",
              "  '초딩',\n",
              "  '##영화',\n",
              "  '##줄',\n",
              "  '.',\n",
              "  '.',\n",
              "  '.',\n",
              "  '.',\n",
              "  '오버',\n",
              "  '##연기',\n",
              "  '##조차',\n",
              "  '가볍',\n",
              "  '##지',\n",
              "  '않',\n",
              "  '##구나'],\n",
              " ['별루', '였다', '.', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT 모델 입력 만들기\n",
        "batch_inputs = tokenizer_bert(\n",
        "    sentences,\n",
        "    padding='max_length',\n",
        "    max_length = 12,\n",
        "    truncation = True\n",
        ")"
      ],
      "metadata": {
        "id": "G1v-g8e-IcB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs['input_ids'] # 토큰 인덱스 시퀀스"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k3XH1CvIkez",
        "outputId": "25cdee61-17a3-40b1-a95a-e4f602464bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 621, 2631, 16, 16, 1993, 3678, 1990, 3323, 3, 0, 0],\n",
              " [2, 997, 16, 16, 16, 2609, 2045, 2796, 1981, 1097, 16, 3],\n",
              " [2, 3274, 9508, 16, 16, 3, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모든 문장 앞에 2, 끝에 3이 붙음\n",
        "  - 각각 [CLS], [SEP]라는 토큰에 대응하는 인덱스"
      ],
      "metadata": {
        "id": "WQ6t_gVdI0cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_inputs['attention_mask']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSIxmXWkIwDi",
        "outputId": "c50bcdf7-c6d1-483c-b483-eb21e3718350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}